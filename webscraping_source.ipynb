{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qKF4GdnP59tb"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wQ9GZj3S3u9q"
      },
      "outputs": [],
      "source": [
        "#title and location I searched for #we can keep searching for multiple locations?\n",
        "title = 'Data Scientist'\n",
        "location = 'San Francisco Bay Area'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gVWHlHeX3xho"
      },
      "outputs": [],
      "source": [
        "#we can change this link for any other platform too but variables will change accordingly. Right now, it is start = 0. We can increase the number to get more data from next pages to. 0 means the first page.\n",
        "list_url = 'https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Data%20Scientist&location=San%20Francisco%20Bay%20Area&geoId=90000084&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0&start=0'\n",
        "response = requests.get(list_url)\n",
        "list_data = response.text\n",
        "list_soup = BeautifulSoup(list_data, 'html.parser')\n",
        "page_jobs = list_soup.find_all('li')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhZubpdE30k0",
        "outputId": "c6ba0321-b4e2-499b-bcf8-7e08ef22b930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4043796448\n",
            "4034434518\n",
            "4024733944\n",
            "3965107368\n",
            "4043793816\n",
            "4039501251\n",
            "4010718955\n",
            "4039095787\n",
            "3982463649\n",
            "4018739243\n"
          ]
        }
      ],
      "source": [
        "id_list = []\n",
        "for job in page_jobs:\n",
        "    base_card_div = job.find('div', {'class': 'base-card'})\n",
        "    job_id = base_card_div.get('data-entity-urn').split(\":\")[3]\n",
        "    print(job_id)\n",
        "    id_list.append(job_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOrvKPB632WY",
        "outputId": "244a2b84-bfca-4c15-af16-1fda654bb95a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['4043796448',\n",
              " '4034434518',\n",
              " '4024733944',\n",
              " '3965107368',\n",
              " '4043793816',\n",
              " '4039501251',\n",
              " '4010718955',\n",
              " '4039095787',\n",
              " '3982463649',\n",
              " '4018739243']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "id_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCPzfhr134rG",
        "outputId": "d9255f8b-03af-4e42-b68c-cafb7ee79f7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "429\n",
            "429\n",
            "429\n",
            "200\n",
            "200\n",
            "200\n",
            "200\n",
            "429\n",
            "200\n",
            "429\n"
          ]
        }
      ],
      "source": [
        "job_list = []\n",
        "\n",
        "for job_id in id_list:\n",
        "    job_url = f'https://www.linkedin.com/jobs-guest/jobs/api/jobPosting/{job_id}'\n",
        "    job_response = requests.get(job_url)\n",
        "    print(job_response.status_code)\n",
        "    job_soup = BeautifulSoup(job_response.text, \"html.parser\")\n",
        "    job_post = {}\n",
        "    try:\n",
        "        job_post[\"job_title\"] = job_soup.find(\"h2\", {\"class\": \"top-card-layout__title font-sans text-lg papabear:text-xl font-bold leading-open text-color-text mb-0 topcard__title\"}).text.strip()\n",
        "    except:\n",
        "        job_post[\"job_title\"] = None\n",
        "    try:\n",
        "        job_post[\"company_name\"] = job_soup.find(\"a\", {\"class\": \"topcard__org-name-link topcard__flavor--black-link\"}).text.strip()\n",
        "    except:\n",
        "        job_post[\"company_name\"] = None\n",
        "    try:\n",
        "        job_post[\"job_level\"] = job_soup.find(\"span\", {\"class\": \"description__job-criteria-text description__job-criteria-text--criteria\"}).text.strip()\n",
        "    except:\n",
        "        job_post[\"job_level\"] = None\n",
        "    try:\n",
        "        job_post[\"city\"] = job_soup.find(\"span\", {\"class\": \"topcard__flavor topcard__flavor--bullet\"}).text.strip()\n",
        "    except:\n",
        "        job_post[\"city\"] = None\n",
        "    try:\n",
        "        job_post[\"base_pay\"] = job_soup.find(\"div\", {\"class\": \"salary compensation__salary\"})\n",
        "    except:\n",
        "        job_post[\"base_pay\"] = None\n",
        "    job_list.append(job_post)\n",
        "\n",
        "#200 means the data is being fetched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hW4fQcn6qa4m",
        "outputId": "8f2f5b7b-c232-4b30-c955-1f3f82a90c9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                   job_title company_name         job_level  \\\n",
            "0                                       None         None              None   \n",
            "1                                       None         None              None   \n",
            "2                                       None         None              None   \n",
            "3                             Data Scientist          Glo  Mid-Senior level   \n",
            "4   Data Scientist Intern, Product Analytics         Meta        Internship   \n",
            "5  Staff Data Scientist, Strategy & Insights     LinkedIn    Not Applicable   \n",
            "6                             Data Scientist         Brex       Entry level   \n",
            "7                                       None         None              None   \n",
            "8                  Machine Learning Engineer      Netflix    Not Applicable   \n",
            "9                                       None         None              None   \n",
            "\n",
            "                city                                         base_pay  \n",
            "0               None                                             None  \n",
            "1               None                                             None  \n",
            "2               None                                             None  \n",
            "3      Daly City, CA  [\\n      $145,000.00/yr - $180,000.00/yr\\n    ]  \n",
            "4     Menlo Park, CA     [\\n      $7,188.00/mo - $11,000.00/mo\\n    ]  \n",
            "5      Sunnyvale, CA  [\\n      $138,000.00/yr - $226,000.00/yr\\n    ]  \n",
            "6  San Francisco, CA                                             None  \n",
            "7               None                                             None  \n",
            "8      Los Gatos, CA  [\\n      $100,000.00/yr - $720,000.00/yr\\n    ]  \n",
            "9               None                                             None  \n"
          ]
        }
      ],
      "source": [
        "job_df = pd.DataFrame(job_list)\n",
        "\n",
        "# Display the DataFrame as a table\n",
        "print(job_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwvqHUxa37sf",
        "outputId": "b78a4fc2-6f20-4066-fdd0-93ea396861b1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "job_list #list of companies on this particular page"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpMrvNPu396J",
        "outputId": "48a0d7de-3ea4-49c4-ca48-e36066c4a82f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ],
      "source": [
        "#Code 2\n",
        "#importing the necessary libraries\n",
        "!pip install beautifulsoup4\n",
        "import urllib.request\n",
        "import requests\n",
        "import urllib.parse\n",
        "import urllib.error\n",
        "import ssl\n",
        "import pandas as pd\n",
        "from urllib.request import Request, urlopen\n",
        "#Importing BeautifulSoup for HTML parsing\n",
        "from bs4 import BeautifulSoup as BS\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbAc5JhLiKVt"
      },
      "outputs": [],
      "source": [
        "#Setting up SSL context to avoid SSL certificate verification errors\n",
        "context = ssl.create_default_context()\n",
        "context.check_hostname = False\n",
        "context.verify_mode = ssl.CERT_NONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "QDj8syxhiL_o",
        "outputId": "10c719bb-e2ac-417a-c622-270e2876bf9d"
      },
      "outputs": [
        {
          "ename": "HTTPError",
          "evalue": "HTTP Error 429: Request denied",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-2a4e5837ed9b>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Perform the HTTP request and parse the HTML content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 429: Request denied"
          ]
        }
      ],
      "source": [
        "url = \"https://www.linkedin.com/jobs/search?keywords=data%20scientist&location=United%20States&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
        "\n",
        "#Set up the request with a custom User-Agent header\n",
        "headers = {  'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36'}\n",
        "request = Request(url, headers=headers)\n",
        "\n",
        "#Perform the HTTP request and parse the HTML content\n",
        "output = urlopen(request).read()\n",
        "soup = BeautifulSoup(output, 'html.parser')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WVktIMHiiNK"
      },
      "outputs": [],
      "source": [
        "url = \"https://www.linkedin.com/jobs/search?keywords=data%20scientist&location=United%20States&geoId=103644278&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
        "response = requests.get(url)\n",
        "list_data = response.text\n",
        "list_soup = BeautifulSoup(list_data, 'html.parser')\n",
        "page_jobs = list_soup.find_all('li')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwXM7ugKilVb"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "\n",
        "listings = soup.find_all('div', class_='base-search-card__info')\n",
        "for i, listing in enumerate(listings[:20]):\n",
        "    job_title = listing.find('h3', class_='base-search-card__title')\n",
        "    job_data = job_title.get_text().strip() if job_title else None\n",
        "\n",
        "    company_name = listing.find('h4', class_='base-search-card__subtitle')\n",
        "    company_data = company_name.get_text().strip() if company_name else None\n",
        "\n",
        "    job_loc = listing.find('span', class_='job-search-card__location')\n",
        "    jobloc_data = job_loc.get_text().strip() if job_loc else None\n",
        "\n",
        "    job_posted = listing.find('time', class_='job-search-card__listdate')\n",
        "    jobposted_data = job_posted if job_posted else None\n",
        "\n",
        "    actively_hiring = listing.find('div', class_ = 'job-posting-benefits text-sm')\n",
        "    hiring_data = \"Yes\" if actively_hiring else \"No\"\n",
        "\n",
        "    url_element = listing.find('a', class_='hidden-nested-link')\n",
        "    url = url_element['href'] if url_element else None\n",
        "\n",
        "\n",
        "    data.append({\"Job Title\": job_data, \"Company Name\": company_data, \"Job Location\": jobloc_data, \"Actively Hiring\": hiring_data, \"URL\": url})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jb5Q4wO_inro"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(data)\n",
        "result.head(20).dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6vqSZDom4Vl"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------------------------------------------------------------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SW6zKoF_nNSE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY7UEBAjnMjs"
      },
      "outputs": [],
      "source": [
        "url = \"https://job-boards.greenhouse.io/lambda/jobs/6093594003\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBsUMpH3m-Mq"
      },
      "outputs": [],
      "source": [
        "def get_page_content(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.content\n",
        "\n",
        "def parse_job_listings(content):\n",
        "    soup = BeautifulSoup(content, \"html.parser\")\n",
        "    job_cards = soup.find_all(\"div\", class_=\"job_seen_beacon\")  # class for job card in Indeed listings\n",
        "\n",
        "    jobs = []\n",
        "    for card in job_cards:\n",
        "        job_title = card.find(\"h2\", class_=\"jobTitle\").text.strip()\n",
        "        company = card.find(\"span\", class_=\"companyName\").text.strip() if card.find(\"span\", class_=\"companyName\") else None\n",
        "        location = card.find(\"div\", class_=\"companyLocation\").text.strip() if card.find(\"div\", class_=\"companyLocation\") else None\n",
        "        salary = card.find(\"div\", class_=\"attribute_snippet\").text.strip() if card.find(\"div\", class_=\"attribute_snippet\") else None\n",
        "        job_link = \"https://www.indeed.com\" + card.find(\"a\", class_=\"jcs-JobTitle\")['href'] if card.find(\"a\", class_=\"jcs-JobTitle\") else None\n",
        "\n",
        "        jobs.append({\n",
        "            \"Job Title\": job_title,\n",
        "            \"Company\": company,\n",
        "            \"Location\": location,\n",
        "            \"Salary\": salary,\n",
        "            \"Job Link\": job_link\n",
        "        })\n",
        "\n",
        "    return jobs\n",
        "\n",
        "\n",
        "\n",
        "def scrape_jobs(url):\n",
        "    page_content = get_page_content(url)\n",
        "    jobs = parse_job_listings(page_content)\n",
        "    return pd.DataFrame(jobs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W7HhrAIndu9",
        "outputId": "e7756f6a-af27-4af9-fb1d-6e6bc320f51c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "job_df = scrape_jobs(url)\n",
        "print(job_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn0FAbkHDioJ"
      },
      "outputs": [],
      "source": [
        "#Kim's Code #Scarpping Job Detail from Indeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-p2Cys-FSjl"
      },
      "outputs": [],
      "source": [
        "#From roadmap\n",
        "# Job Title:\n",
        "# Data Scientist, Data Analyst,\n",
        "# Quantitative Analyst, Quantitative Researcher, Quantitative Research Analyst,\n",
        "# Business Analyst, and Business Intelligence Analyst\n",
        "\n",
        "# Job Attributes:\n",
        "# Job Title (str), Job Description (str), Salary Range (int/float), Location(str), (bool)Remote, Industry, Software Skills (str), Employment Type (str)\n",
        "\n",
        "# Adjusted Job Attributes: Job Title(obj), Company Name(obj), Full Location(obj), State(obj), Remote(bool) ,\n",
        "# Salary Min(float), Salary Max(float), Employment Type(obj), Industry(obj), Software Skills(obj), urls(obj)\n",
        "\n",
        "\n",
        "# Assumptions:\n",
        "#     1. Employment type is full time unless data said otherwise\n",
        "#     2. Hourly salary is converted to yearly = 40h/week * 52w/year or Monthly salary * 52 w/year\n",
        "#     3. Location under job title is the hiring location\n",
        "#     4. If only one salary is provided, salary min = salary max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iqmKQ9X0H-mF",
        "outputId": "80d03d1d-892b-439b-e17a-d0663e425201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
            "Collecting trio~=0.17 (from selenium)\n",
            "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting trio-websocket~=0.9 (from selenium)\n",
            "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (2024.8.30)\n",
            "Requirement already satisfied: typing_extensions~=4.9 in /usr/local/lib/python3.10/dist-packages (from selenium) (4.12.2)\n",
            "Requirement already satisfied: websocket-client~=1.8 in /usr/local/lib/python3.10/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (3.10)\n",
            "Collecting outcome (from trio~=0.17->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from trio~=0.17->selenium) (1.2.2)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: outcome, h11, wsproto, trio, trio-websocket, selenium\n",
            "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from webdriver-manager) (24.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->webdriver-manager) (2024.8.30)\n",
            "Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, webdriver-manager\n",
            "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.2\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Ign:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,370 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,590 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,596 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,159 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,447 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,318 kB]\n",
            "Fetched 18.7 MB in 4s (4,692 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  apparmor chromium-browser libfuse3-3 liblzo2-2 libudev1 snapd squashfs-tools systemd-hwe-hwdb\n",
            "  udev\n",
            "Suggested packages:\n",
            "  apparmor-profiles-extra apparmor-utils fuse3 zenity | kdialog\n",
            "The following NEW packages will be installed:\n",
            "  apparmor chromium-browser chromium-chromedriver libfuse3-3 liblzo2-2 snapd squashfs-tools\n",
            "  systemd-hwe-hwdb udev\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 9 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 28.5 MB of archives.\n",
            "After this operation, 118 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 apparmor amd64 3.0.4-2ubuntu2.4 [598 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 liblzo2-2 amd64 2.10-2build3 [53.7 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 squashfs-tools amd64 1:4.5-3build1 [159 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.12 [78.2 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 udev amd64 249.11-0ubuntu3.12 [1,557 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfuse3-3 amd64 3.10.5-1build1 [81.2 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 snapd amd64 2.63+22.04ubuntu0.1 [25.9 MB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-browser amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [49.2 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 chromium-chromedriver amd64 1:85.0.4183.83-0ubuntu2.22.04.1 [2,308 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 systemd-hwe-hwdb all 249.11.5 [3,228 B]\n",
            "Fetched 28.5 MB in 1s (34.5 MB/s)\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package apparmor.\n",
            "(Reading database ... 123620 files and directories currently installed.)\n",
            "Preparing to unpack .../apparmor_3.0.4-2ubuntu2.4_amd64.deb ...\n",
            "Unpacking apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Selecting previously unselected package liblzo2-2:amd64.\n",
            "Preparing to unpack .../liblzo2-2_2.10-2build3_amd64.deb ...\n",
            "Unpacking liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Selecting previously unselected package squashfs-tools.\n",
            "Preparing to unpack .../squashfs-tools_1%3a4.5-3build1_amd64.deb ...\n",
            "Unpacking squashfs-tools (1:4.5-3build1) ...\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.12) over (249.11-0ubuntu3.10) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package udev.\n",
            "(Reading database ... 123828 files and directories currently installed.)\n",
            "Preparing to unpack .../udev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking udev (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libfuse3-3:amd64.\n",
            "Preparing to unpack .../libfuse3-3_3.10.5-1build1_amd64.deb ...\n",
            "Unpacking libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Selecting previously unselected package snapd.\n",
            "Preparing to unpack .../snapd_2.63+22.04ubuntu0.1_amd64.deb ...\n",
            "Unpacking snapd (2.63+22.04ubuntu0.1) ...\n",
            "Setting up apparmor (3.0.4-2ubuntu2.4) ...\n",
            "Created symlink /etc/systemd/system/sysinit.target.wants/apparmor.service → /lib/systemd/system/apparmor.service.\n",
            "Setting up liblzo2-2:amd64 (2.10-2build3) ...\n",
            "Setting up squashfs-tools (1:4.5-3build1) ...\n",
            "Setting up udev (249.11-0ubuntu3.12) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up libfuse3-3:amd64 (3.10.5-1build1) ...\n",
            "Setting up snapd (2.63+22.04ubuntu0.1) ...\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.apparmor.service → /lib/systemd/system/snapd.apparmor.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.autoimport.service → /lib/systemd/system/snapd.autoimport.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.core-fixup.service → /lib/systemd/system/snapd.core-fixup.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.recovery-chooser-trigger.service → /lib/systemd/system/snapd.recovery-chooser-trigger.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Created symlink /etc/systemd/system/cloud-final.service.wants/snapd.seeded.service → /lib/systemd/system/snapd.seeded.service.\n",
            "Unit /lib/systemd/system/snapd.seeded.service is added as a dependency to a non-existent unit cloud-final.service.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/snapd.service → /lib/systemd/system/snapd.service.\n",
            "Created symlink /etc/systemd/system/timers.target.wants/snapd.snap-repair.timer → /lib/systemd/system/snapd.snap-repair.timer.\n",
            "Created symlink /etc/systemd/system/sockets.target.wants/snapd.socket → /lib/systemd/system/snapd.socket.\n",
            "Created symlink /etc/systemd/system/final.target.wants/snapd.system-shutdown.service → /lib/systemd/system/snapd.system-shutdown.service.\n",
            "Selecting previously unselected package chromium-browser.\n",
            "(Reading database ... 124058 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-browser_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "=> Installing the chromium snap\n",
            "==> Checking connectivity with the snap store\n",
            "===> System doesn't have a working snapd, skipping\n",
            "Unpacking chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_1%3a85.0.4183.83-0ubuntu2.22.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Selecting previously unselected package systemd-hwe-hwdb.\n",
            "Preparing to unpack .../systemd-hwe-hwdb_249.11.5_all.deb ...\n",
            "Unpacking systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up systemd-hwe-hwdb (249.11.5) ...\n",
            "Setting up chromium-browser (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (1:85.0.4183.83-0ubuntu2.22.04.1) ...\n",
            "Processing triggers for udev (249.11-0ubuntu3.12) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for dbus (1.12.20-2ubuntu4.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium\n",
        "!pip install webdriver-manager\n",
        "!pip install beautifulsoup4\n",
        "!pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InEYYPjvnFPh"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "\n",
        "user_agents = [\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.1 Safari/605.1.15'\n",
        "]\n",
        "\n",
        "def setup_driver():\n",
        "\n",
        "    options = Options()\n",
        "    options.add_argument(\"--headless\")\n",
        "    options.add_argument(\"--no-sandbox\")\n",
        "    options.add_argument(\"--disable-dev-shm-usage\")\n",
        "    options.add_argument(f\"user-agent={random.choice(user_agents)}\")\n",
        "\n",
        "    service = Service(ChromeDriverManager().install())\n",
        "    driver = webdriver.Chrome(service=service, options=options)\n",
        "\n",
        "    return driver\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxozUKBjEZDP"
      },
      "outputs": [],
      "source": [
        "# Scrape job detail in each job urls\n",
        "def scrape_indeed_job_details(driver, url):\n",
        "    driver.get(url)\n",
        "\n",
        "    try:\n",
        "        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID, \"jobDescriptionText\")))\n",
        "    except:\n",
        "        print(f\"Timeout waiting for page to load: {url}\")\n",
        "        return None\n",
        "\n",
        "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "    job_details = {\n",
        "        'Job Title': 'N/A',\n",
        "        'Company Name': 'N/A',\n",
        "        'Full Location': 'N/A',\n",
        "        'State': 'N/A',\n",
        "        'Remote': False,\n",
        "        'Salary Min': None,\n",
        "        'Salary Max': None,\n",
        "        'Employment Type': ['Fulltime'],#Assumption 1: Employment type is full time unless data said otherwise\n",
        "        'Industry': 'N/A',\n",
        "        'Software Skills': 'N/A',\n",
        "        'Job Description': 'N/A',\n",
        "        'URL': url\n",
        "    }\n",
        "\n",
        "    # Job Title\n",
        "    job_title_elem = soup.find('h1', class_='jobsearch-JobInfoHeader-title')\n",
        "    if job_title_elem:\n",
        "        job_details['Job Title'] = job_title_elem.text.strip()\n",
        "\n",
        "    # Company Name\n",
        "    company_elem = soup.find('a', class_='css-1ioi40n e19afand0')\n",
        "    if company_elem:\n",
        "        job_details['Company Name'] = company_elem.text.strip()\n",
        "\n",
        "    # Full Location (location 1)\n",
        "    location_elem = soup.find('div', {'data-testid': 'job-location'})\n",
        "    if location_elem:\n",
        "        job_details['Full Location'] = location_elem.text.strip()\n",
        "\n",
        "    # Full Location (location 2, only if location 1 resulted in 'N/A')\n",
        "    if job_details['Full Location'] == 'N/A':\n",
        "        location_elem = soup.find('div', class_='css-waniwe')\n",
        "        if location_elem:\n",
        "            job_details['Full Location'] = location_elem.text.strip()\n",
        "\n",
        "    #Full Location (Location 3, remote)\n",
        "    if job_details['Full Location'] == 'N/A':\n",
        "        remote_elem = soup.find('div', class_='css-17cdm7w eu4oa1w0')\n",
        "        if remote_elem and remote_elem.text.strip().lower() == 'remote':\n",
        "            job_details['Full Location'] = 'Remote'\n",
        "\n",
        "    # State\n",
        "    if job_details['Full Location'] != 'N/A':\n",
        "            state_match = re.search(r',\\s*([A-Z]{2})\\b', job_details['Full Location'])\n",
        "            if state_match:\n",
        "                job_details['State'] = state_match.group(1)\n",
        "\n",
        "    # Job Description\n",
        "    job_description_elem = soup.find('div', id='jobDescriptionText')\n",
        "    if job_description_elem:\n",
        "        job_details['Job Description'] = job_description_elem.text.strip()\n",
        "\n",
        "    # Remote/Hybrid work\n",
        "    location_text = job_details['Full Location'].lower()\n",
        "    description_text = job_details['Job Description'].lower()\n",
        "    job_details['Remote'] = any(keyword in location_text or keyword in description_text\n",
        "                                for keyword in ['remote', 'hybrid', 'work from home', 'wfh'])\n",
        "\n",
        "    # Salary and\n",
        "    salary_job_type_elem = soup.find('div', id='salaryInfoAndJobType')\n",
        "    if salary_job_type_elem:\n",
        "        salary_span = salary_job_type_elem.find('span', class_='css-19j1a75')\n",
        "        employment_type_span = salary_job_type_elem.find('span', class_='css-k5flys')\n",
        "\n",
        "        if salary_span:\n",
        "            salary_text = salary_span.text.strip()\n",
        "            salary_numbers = re.findall(r'\\$[\\d.,]+', salary_text)\n",
        "            is_hourly = 'hour' in salary_text.lower() or 'hr' in salary_text.lower()\n",
        "            is_monthly = 'month' in salary_text.lower()\n",
        "\n",
        "            if len(salary_numbers) == 2:\n",
        "                min_salary = float(salary_numbers[0].replace('$', '').replace(',', ''))\n",
        "                max_salary = float(salary_numbers[1].replace('$', '').replace(',', ''))\n",
        "                if is_hourly:\n",
        "                    # Convert hourly to yearly\n",
        "                    job_details['Salary Min'] = round(min_salary * 40 * 52, -3)\n",
        "                    job_details['Salary Max'] = round(max_salary * 40 * 52, -3)\n",
        "                elif is_monthly:\n",
        "                    # Convert monthly to yearly\n",
        "                    job_details['Salary Min'] = round(min_salary * 12, -3)\n",
        "                    job_details['Salary Max'] = round(max_salary * 12, -3)\n",
        "                else:\n",
        "                    job_details['Salary Min'] = round(min_salary, -3)\n",
        "                    job_details['Salary Max'] = round(max_salary, -3)\n",
        "            elif len(salary_numbers) == 1:\n",
        "                salary = float(salary_numbers[0].replace('$', '').replace(',', ''))\n",
        "                if is_hourly:\n",
        "                    # Convert hourly to yearly\n",
        "                    yearly_salary = round(salary * 40 * 52, -3)\n",
        "                    job_details['Salary Min'] = job_details['Salary Max'] = yearly_salary\n",
        "                elif is_monthly:\n",
        "                    # Convert monthly to yearly\n",
        "                    yearly_salary = round(salary * 12, -3)\n",
        "                    job_details['Salary Min'] = job_details['Salary Max'] = yearly_salary\n",
        "                else:\n",
        "                    job_details['Salary Min'] = job_details['Salary Max'] = round(salary, -3)\n",
        "        # Employment Type\n",
        "        if employment_type_span:\n",
        "            job_details['Employment Type'] = [employment_type_span.text.strip().replace('-', '').strip()]\n",
        "\n",
        "    # Industry Extraction\n",
        "#     if job_details['Company Name'] != 'N/A':\n",
        "#             job_details['Industry'] = get_industry_from_company(driver, job_details['Company Name'])\n",
        "\n",
        "    # Software Skills >> add more if needed\n",
        "    common_skills = ['Python', 'SQL', 'Excel', 'Tableau', 'Power BI', 'R', 'SAS', 'SPSS', 'Java', 'C++']\n",
        "#     job_details['Software Skills'] = ', '.join([skill for skill in common_skills if skill.lower() in job_details['Job Description'].lower()])\n",
        "    job_details['Software Skills'] = [skill for skill in common_skills if skill.lower() in job_details['Job Description'].lower()]\n",
        "\n",
        "    return job_details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBrIHRUjEgAL"
      },
      "outputs": [],
      "source": [
        "#scrape job urls from indeed search page\n",
        "def scrape_job_urls(base_url, num_pages, driver):\n",
        "    job_urls = []\n",
        "\n",
        "    for page in range(num_pages):\n",
        "        url = f\"{base_url}&start={page * 10}\"\n",
        "        driver.get(url)\n",
        "\n",
        "        try:\n",
        "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.CLASS_NAME, \"job_seen_beacon\")))\n",
        "        except:\n",
        "            print(f\"Timeout waiting for page to load: {url}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        job_cards = soup.find_all('div', class_='job_seen_beacon')\n",
        "\n",
        "        for card in job_cards:\n",
        "            job_link = card.find('a', class_='jcs-JobTitle')\n",
        "            if job_link and 'href' in job_link.attrs:\n",
        "                job_url = 'https://www.indeed.com' + job_link['href'].split('&from')[0]\n",
        "                job_urls.append({'URL': job_url, 'Base Search URL': base_url})\n",
        "\n",
        "        print(f\"Scraped {len(job_urls)} job URLs so far...\")\n",
        "        time.sleep(random.uniform(2, 5))  # Random delay between page loads\n",
        "\n",
        "    return job_urls\n",
        "\n",
        "def scrape_multiple_base_urls(base_urls, num_pages_per_url):\n",
        "    driver = setup_driver()\n",
        "    all_job_urls = []\n",
        "\n",
        "    for base_url in base_urls:\n",
        "        print(f\"Scraping job URLs for: {base_url}\")\n",
        "        job_urls = scrape_job_urls(base_url, num_pages_per_url, driver)\n",
        "        all_job_urls.extend(job_urls)\n",
        "        print(f\"Total job URLs scraped so far: {len(all_job_urls)}\")\n",
        "        time.sleep(random.uniform(5, 10))  # Delay between base URLs\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Create DataFrame of scraped URLs\n",
        "    url_df = pd.DataFrame(all_job_urls)\n",
        "    return url_df\n",
        "\n",
        "def scrape_multiple_jobs(url_df):\n",
        "    driver = setup_driver()\n",
        "    all_job_details = []\n",
        "\n",
        "    for i, row in url_df.iterrows():\n",
        "        url = row['URL']\n",
        "        print(f\"Scraping job {i+1} of {len(url_df)}\")\n",
        "        try:\n",
        "            job_details = scrape_indeed_job_details(driver, url)\n",
        "            if job_details:\n",
        "                all_job_details.append(job_details)\n",
        "\n",
        "            # Add a random delay between requests to avoid overloading the server\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "        except Exception as e:\n",
        "            print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "    driver.quit()\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(all_job_details)\n",
        "    column_order = ['Job Title','Company Name', 'Full Location', 'State', 'Remote', 'Salary Min', 'Salary Max', 'Employment Type',\n",
        "                    'Industry', 'Software Skills', 'Job Description', 'URL']\n",
        "    df = df[column_order]\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOB6XXn8FMBy",
        "outputId": "4f972130-2c46-424c-d67f-33590749e9f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:WDM:====== WebDriver manager ======\n",
            "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
            "INFO:WDM:About to download new driver from https://chromedriver.storage.googleapis.com/114.0.5735.90/chromedriver_linux64.zip\n",
            "INFO:WDM:Driver downloading response is 200\n",
            "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
            "INFO:WDM:Get LATEST chromedriver version for google-chrome\n",
            "INFO:WDM:Driver has been saved in cache [/root/.wdm/drivers/chromedriver/linux64/114.0.5735.90]\n",
            "ERROR:WebDriverManager:An error occurred while setting up the driver: Message: unknown error: Chrome failed to start: exited abnormally.\n",
            "  (unknown error: DevToolsActivePort file doesn't exist)\n",
            "  (The process started from chrome location /usr/bin/chromium-browser is no longer running, so ChromeDriver is assuming that Chrome has crashed.)\n",
            "Stacktrace:\n",
            "#0 0x5af2cff2b4e3 <unknown>\n",
            "#1 0x5af2cfc5ac76 <unknown>\n",
            "#2 0x5af2cfc83d78 <unknown>\n",
            "#3 0x5af2cfc80029 <unknown>\n",
            "#4 0x5af2cfcbeccc <unknown>\n",
            "#5 0x5af2cfcbe47f <unknown>\n",
            "#6 0x5af2cfcb5de3 <unknown>\n",
            "#7 0x5af2cfc8b2dd <unknown>\n",
            "#8 0x5af2cfc8c34e <unknown>\n",
            "#9 0x5af2cfeeb3e4 <unknown>\n",
            "#10 0x5af2cfeef3d7 <unknown>\n",
            "#11 0x5af2cfef9b20 <unknown>\n",
            "#12 0x5af2cfef0023 <unknown>\n",
            "#13 0x5af2cfebe1aa <unknown>\n",
            "#14 0x5af2cff146b8 <unknown>\n",
            "#15 0x5af2cff14847 <unknown>\n",
            "#16 0x5af2cff24243 <unknown>\n",
            "#17 0x78a212b9cac3 <unknown>\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping job URLs for: https://www.indeed.com/jobs?q=Business+Intelligence+Analyst&l=\n",
            "An error occurred: 'NoneType' object has no attribute 'get'\n"
          ]
        }
      ],
      "source": [
        "#Main execution 7 search page urls of 7 provided job titles\n",
        "base_urls = [\n",
        "    \"https://www.indeed.com/jobs?q=data+analyst&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=data+scientist&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=Quantitative+Analyst&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=Quantitative+Researcher&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=Quantitative+Research+Analyst&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=business+analyst&l=\",\n",
        "    \"https://www.indeed.com/jobs?q=Business+Intelligence+Analyst&l=\"\n",
        "] # Can add more search page urls for different job titles\n",
        "num_pages_per_url = 7  # Adjust this to scrape more or fewer pages per URL\n",
        "\n",
        "try:\n",
        "    # First, scrape and store all job URLs\n",
        "    url_df = scrape_multiple_base_urls(base_urls, num_pages_per_url)\n",
        "\n",
        "    # Save the URL DataFrame to CSV\n",
        "    url_df.to_csv('indeed_job_urls.csv', index=False)\n",
        "    print(\"Job URLs saved to 'indeed_job_urls.csv'\")\n",
        "\n",
        "    # Now, scrape job details using the URL DataFrame\n",
        "    results_df = scrape_multiple_jobs(url_df)\n",
        "\n",
        "    # Save the job details to CSV\n",
        "    results_df.to_csv('indeed_job_listings_multi_url.csv', index=False)\n",
        "    print(\"Job details saved to 'indeed_job_listings_multi_url.csv'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUVsryDgFPSe"
      },
      "outputs": [],
      "source": [
        "results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn3beYE3EJOM"
      },
      "outputs": [],
      "source": [
        "# Scrap Industry from Company name from Dataframe\n",
        "def get_industry_from_company(driver, company_name):\n",
        "    encoded_company = company_name.replace(' ', '+')\n",
        "    url = f\"https://www.indeed.com/companies/search?q={encoded_company}\"\n",
        "\n",
        "    try:\n",
        "        driver.get(url)\n",
        "        WebDriverWait(driver, 20).until(\n",
        "            EC.presence_of_element_located((By.CSS_SELECTOR, \".css-1difit4.e1wnkr790, .css-1h2acbs\"))\n",
        "        )\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "        industry_div = soup.find('div', class_='css-1difit4 e1wnkr790')\n",
        "\n",
        "        if industry_div:\n",
        "            return industry_div.text.strip()\n",
        "        else:\n",
        "            no_results = soup.find('div', class_='css-1h2acbs')\n",
        "            if no_results:\n",
        "                return \"No industry information found\"\n",
        "            else:\n",
        "                return \"Industry not found\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching for {company_name}\")\n",
        "        return \"N/A\"\n",
        "\n",
        "def update_dataframe_with_industry(df):\n",
        "    driver = setup_driver()\n",
        "    total_companies = len(df)\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        company_name = row['Company Name']\n",
        "        if pd.notna(company_name) and company_name != 'N/A':\n",
        "            print(f\"({index + 1} of {total_companies}) Searching industry for {company_name} \")\n",
        "            industry = get_industry_from_company(driver, company_name)\n",
        "            df.at[index, 'Industry'] = industry\n",
        "\n",
        "            # Add a random delay to avoid overloading the server\n",
        "            time.sleep(random.uniform(2, 5))\n",
        "\n",
        "    driver.quit()\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TUzoOtbvEJVK"
      },
      "outputs": [],
      "source": [
        "# Update the DataFrame with industry information\n",
        "updated_df = update_dataframe_with_industry(results_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLYcNT4tLAza"
      },
      "outputs": [],
      "source": [
        "updated_df.to_csv('indeed_job_listings_multi_url_updated.csv', index=False)\n",
        "updated_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
